{"cells":[{"cell_type":"markdown","metadata":{},"source":["## In this Jupyter Notebook, I am practing creating a CycleGAN deep learning model. I hope that this will give me a better understanding of making CycleGAN for usage in The FaRi Project.\n","\n","### To-Do:\n","###     - Check each part of the architecture with print statements to ensure every part is working as expected to debug errors in executing correctly on Google Colaboratory."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# IMPORTS\n","import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import numpyÂ  as np\n","import scipy.ndimage\n","from IPython.display import clear_output\n","tf.config.run_functions_eagerly(True)\n","\n","# Training parameters\n","learning_rate = 0.0001\n","batch_size = 32\n","epochs = 25000\n","\n","# Network parameters\n","image_dimension = 784  # Image size is 28 x 28\n","\n","# Discriminator nodes\n","\"\"\"\n","This hidden dimension dictates that we are making a neural network\n","with a hidden dimension with 128 nodes. This helps in creating\n","the architecture of the neural networks used for discrimination and\n","generation\n","\n","For the Discriminators, this is the architecture of the neural network\n","\n","              INPUT         HIDDEN LAYER          OUTPUT\n","            784 Nodes         128 Nodes           1 Node\n","\n","For the Generators, this is the architecture of the neural networks\n","\n","              INPUT         HIDDEN LAYER          OUTPUT\n","            784 Nodes         128 Nodes           784 Nodes*\n","\n","\n","We have 784 input nodes since we are taking in the pixels of the image,\n","similarly to a handwritten digit classifier. Similarly, the 784 output values\n","correspond to every pixel that makes up the generated output image.\n","\n","For an actual implementation, we would want to make this architecture more\n","advanced using a CNN and not a neural network with one layer\n","\"\"\"\n","H_dim = 128\n","\n","\"\"\"\n","This function is what we are using in order to initialize the Weights and\n","Biases for our Discriminator and our Generator.\n","\"\"\"\n","def xavier_init(shape):\n","    initializer = tf.keras.initializers.GlorotNormal()\n","    return tf.Variable(initializer(shape=shape), trainable=True)\n","\n","\n","\n","\"\"\"\n","CycleGAN requires two sets of inputs from the outside in order to train on\n","which will correspond to the placeholders that we see below. Initializing\n","the shape with None for the amount of examples because we do not know how many\n","inputs we are going to be receiving from our placeholders. However, we do know\n","that each example is going to have 784 features corresponding to the amount\n","of pixels in the image\n","\"\"\"\n","\n","X_A = tf.Variable(tf.random.normal([batch_size, image_dimension], dtype=tf.float32))\n","X_B = tf.Variable(tf.random.normal([batch_size, image_dimension], dtype=tf.float32))\n","\n","\"\"\"\n","Here we define our weights and bias dictionaries for Discriminator A\n","\"\"\"\n","\n","Disc_A_W = {\n","    # This key corresponds to the Weights for hidden layer\n","    \"disc_H\" : tf.Variable(xavier_init([image_dimension, H_dim])),\n","    # This key corresponds to the Weights for the output layer\n","    \"disc_final\": tf.Variable(xavier_init([H_dim, 1]))\n","}\n","\n","Disc_A_B = {\n","    #This key corresponds to the Biases for the hidden layer\n","    \"disc_H\": tf.Variable(xavier_init([H_dim])),\n","    # This key corresponds to the Biases for the output layer\n","    \"disc_final\": tf.Variable(xavier_init([1]))\n","}\n","\n","\"\"\"\n","Here we define our weights and bias dictionaries for Discriminator B\n","\"\"\"\n","\n","Disc_B_W = {\n","    # This key corresponds to the Weights for hidden layer\n","    \"disc_H\" : tf.Variable(xavier_init([image_dimension, H_dim])),\n","    # This key corresponds to the Weights for the output layer\n","    \"disc_final\": tf.Variable(xavier_init([H_dim, 1]))\n","}\n","\n","Disc_B_B = {\n","    #This key corresponds to the Biases for the hidden layer\n","    \"disc_H\": tf.Variable(xavier_init([H_dim])),\n","    # This key corresponds to the Biases for the output layer\n","    \"disc_final\": tf.Variable(xavier_init([1]))\n","}\n","\n","\"\"\"\n","Here we define our weights and bias dictionaries for Generator transforming\n","placeholder A to B\n","\"\"\"\n","\n","Gen_AB_W = {\n","    # This key corresponds to the Weights for hidden layer\n","    \"Gen_H\" : tf.Variable(xavier_init([image_dimension, H_dim])),\n","    # This key corresponds to the Weights for the output layer\n","    \"Gen_final\": tf.Variable(xavier_init([H_dim, image_dimension]))\n","}\n","\n","Gen_AB_B = {\n","    #This key corresponds to the Biases for the hidden layer\n","    \"Gen_H\": tf.Variable(xavier_init([H_dim])),\n","    # This key corresponds to the Biases for the output layer\n","    \"Gen_final\": tf.Variable(xavier_init([image_dimension]))\n","}\n","\n","\"\"\"\n","Here we define our weights and bias dictionaries for Generator transforming\n","placeholder B to A\n","\"\"\"\n","\n","Gen_BA_W = {\n","    # This key corresponds to the Weights for hidden layer\n","    \"Gen_H\" : tf.Variable(xavier_init([image_dimension, H_dim])),\n","    # This key corresponds to the Weights for the output layer\n","    \"Gen_final\": tf.Variable(xavier_init([H_dim, image_dimension]))\n","}\n","\n","Gen_BA_B = {\n","    #This key corresponds to the Biases for the hidden layer\n","    \"Gen_H\": tf.Variable(xavier_init([H_dim])),\n","    # This key corresponds to the Biases for the output layer\n","    \"Gen_final\": tf.Variable(xavier_init([image_dimension]))\n","}\n","\n","\n","\n","\"\"\"\n","Now we want to actually create the single hidden layer Discriminators and\n","Generators using TensorFlow, so we do that using the functions below.\n","\"\"\"\n","\n","def Disc_A(x):\n","    # Cast input to float32 to avoid type mismatch\n","    x = tf.cast(x, tf.float32)\n","\n","    # Here we are getting the hidden layer's predicted outputs using Y = wX + b\n","    hidden_layer_pred = tf.nn.leaky_relu(tf.add(tf.matmul(x, Disc_A_W[\"disc_H\"]),\n","                                          Disc_A_B[\"disc_H\"]))\n","    \n","    # Add batch normalization\n","    hidden_layer_pred = tf.keras.layers.BatchNormalization()(hidden_layer_pred)\n","\n","    # Here we are getting the output's predicted value(s)\n","    output_layer_pred = tf.nn.sigmoid(tf.add(tf.matmul(hidden_layer_pred,\n","                                                      Disc_A_W[\"disc_final\"]),\n","                                             Disc_A_B[\"disc_final\"]))\n","\n","    return output_layer_pred\n","\n","def Disc_B(x):\n","    # Cast input to float32 to avoid type mismatch\n","    x = tf.cast(x, tf.float32)\n","\n","    # Here are the predicted values for the hidden layer using Y = wX + b\n","    hidden_layer_pred = tf.nn.leaky_relu(tf.add(tf.matmul(x, Disc_B_W[\"disc_H\"]),\n","                                          Disc_B_B[\"disc_H\"]))\n","    \n","    # Add batch normalization\n","    hidden_layer_pred = tf.keras.layers.BatchNormalization()(hidden_layer_pred)\n","\n","    # Here we get the output layer's predicted value(s)\n","    output_layer_pred = tf.nn.sigmoid(tf.add(tf.matmul(hidden_layer_pred,\n","                                                      Disc_B_W[\"disc_final\"]),\n","                                             Disc_B_B[\"disc_final\"]))\n","\n","    return output_layer_pred\n","\n","def Gen_AB(x):\n","    # Cast input to float32 to avoid type mismatch\n","    x = tf.cast(x, tf.float32)\n","\n","    # Here are the predicted values for the hidden layer using Y = wX + b\n","    hidden_layer_pred = tf.nn.leaky_relu(tf.add(tf.matmul(x, Gen_AB_W[\"Gen_H\"]),\n","                                          Gen_AB_B[\"Gen_H\"]))\n","    \n","    # Add batch normalization\n","    hidden_layer_pred = tf.keras.layers.BatchNormalization()(hidden_layer_pred)\n","\n","    # Here we get the output layer's predicted value(s)\n","    output_layer_pred = tf.nn.sigmoid(tf.add(tf.matmul(hidden_layer_pred,\n","                                                      Gen_AB_W[\"Gen_final\"]),\n","                                             Gen_AB_B[\"Gen_final\"]))\n","\n","    return output_layer_pred\n","\n","def Gen_BA(x):\n","    # Cast input to float32 to avoid type mismatch\n","    x = tf.cast(x, tf.float32)\n","\n","    # Here are the predicted values for the hidden layer using Y = wX + b\n","    hidden_layer_pred = tf.nn.leaky_relu(tf.add(tf.matmul(x, Gen_BA_W[\"Gen_H\"]),\n","                                          Gen_BA_B[\"Gen_H\"]))\n","    \n","    # Add batch normalization\n","    hidden_layer_pred = tf.keras.layers.BatchNormalization()(hidden_layer_pred)\n","\n","    # Here we get the output layer's predicted value(s)\n","    output_layer_pred = tf.nn.sigmoid(tf.add(tf.matmul(hidden_layer_pred,\n","                                                      Gen_BA_W[\"Gen_final\"]),\n","                                             Gen_BA_B[\"Gen_final\"]))\n","\n","    return output_layer_pred\n","\n","\"\"\"\n","Now let us actually build the CycleGAN Network\n","\n","First, we begin by creating the GAN (Generative Adversarial Network)\n","for approximating A's distribution. In order to do this, we use B to A\n","Generator that we created, trained on input B. Then, we train the\n","Discriminator on real input from input A as well as fake input from the\n","B to A input we generated.\n","\"\"\"\n","X_BA = Gen_BA(X_B)\n","Disc_A_real = Disc_A(X_A)\n","Disc_A_fake = Disc_A(X_BA)\n","\n","\"\"\"\n","Then, we create the GAN (Generative Adversarial Network) for approximating\n","B's distribution. In order to do this, we use A to B Generator that we\n","created, trained on input A. Then, we train the Discriminator on real input\n","from input B as well as fake input from the A to B input we generated.\n","\"\"\"\n","X_AB = Gen_AB(X_A)\n","Disc_B_real = Disc_B(X_B)\n","Disc_B_fake = Disc_B(X_AB)\n","\n","\"\"\"\n","Now we calculate the Discriminator Loss Function.\n","\"\"\"\n","# Inject label noise by adding small random values to real and fake labels\n","real_labels = tf.ones_like(Disc_A_real) + 0.05 * tf.random.uniform(tf.shape(Disc_A_real))\n","fake_labels = tf.zeros_like(Disc_A_fake) + 0.05 * tf.random.uniform(tf.shape(Disc_A_fake))\n","\n","# Use these noisy labels in the loss function\n","Loss_Disc_A = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_A_real, labels=real_labels)) + \\\n","              tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_A_fake, labels=fake_labels))\n","\n","# Repeat similarly for Disc_B:\n","real_labels_B = tf.ones_like(Disc_B_real) + 0.05 * tf.random.uniform(tf.shape(Disc_B_real))\n","fake_labels_B = tf.zeros_like(Disc_B_fake) + 0.05 * tf.random.uniform(tf.shape(Disc_B_fake))\n","\n","Loss_Disc_B = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_B_real, labels=real_labels_B)) + \\\n","              tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_B_fake, labels=fake_labels_B))\n","\n","Disc_Loss = Loss_Disc_A + Loss_Disc_B\n","\n","# Image Reconstruction\n","\"\"\"\n","We need to reconstruct the images back to their original input through the\n","Generators once more to figure out the reconstruction loss to close the cycle\n","of the CycleGAN. This in conjunction with the Generator's own loss will make\n","up the overall Generator loss\n","\"\"\"\n","X_BAB = Gen_AB(X_BA)\n","X_ABA = Gen_BA(X_AB)\n","\n","# Generator Loss Function\n","\"\"\"\n","The Generator Loss Function is the Mean Squared Error of the output of the\n","Discriminator discriminating against fake input minus a vector of the same\n","shape made up of 1s.\n","\"\"\"\n","Loss_Gen_A = tf.reduce_mean(tf.square(Disc_B_fake - tf.ones_like(Disc_B_fake)))\n","Loss_Gen_B = tf.reduce_mean(tf.square(Disc_A_fake - tf.ones_like(Disc_A_fake)))\n","Loss_total = Loss_Gen_A + Loss_Gen_B\n","\n","# Reconstruction Loss for CycleGAN\n","\"\"\"\n","Here we use the L1 norm for reconstruction loss.\n","\"\"\"\n","Loss_recon_A = tf.reduce_mean(10 * tf.abs(X_A - X_ABA))\n","Loss_recon_B = tf.reduce_mean(10 * tf.abs(X_B - X_BAB))\n","Loss_recon_total = Loss_recon_A + Loss_recon_B\n","\n","Gen_Loss = Loss_total + Loss_recon_total\n","\n","# Parameters list of Discriminator\n","Disc_param = [Disc_A_W[\"disc_H\"], Disc_A_W[\"disc_final\"], Disc_A_B[\"disc_H\"],\n","              Disc_A_B[\"disc_final\"], Disc_B_W[\"disc_H\"], Disc_B_W[\"disc_final\"],\n","              Disc_B_B[\"disc_H\"], Disc_B_B[\"disc_final\"]]\n","\n","# Parameters list of Generator\n","Gen_param = [Gen_AB_W[\"Gen_H\"], Gen_AB_W[\"Gen_final\"], Gen_AB_B[\"Gen_H\"],\n","             Gen_AB_B[\"Gen_final\"], Gen_BA_W[\"Gen_H\"], Gen_BA_W[\"Gen_final\"],\n","             Gen_BA_B[\"Gen_H\"], Gen_BA_B[\"Gen_final\"]]\n","\n","# Optimizers\n","Gen_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","Disc_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n","\n","\n","\n","def train_step(X_A_batch, X_B_batch):\n","  X_A_batch = tf.cast(X_A_batch, tf.float32)\n","  X_A_batch = tf.reshape(X_A_batch, [batch_size, 784])  # Flatten to 784 dimensions\n","\n","  X_B_batch = tf.cast(X_B_batch, tf.float32)\n","  X_B_batch = tf.reshape(X_B_batch, [batch_size, 784])  # Flatten to 784 dimensions\n","\n","  print(\"X_A_batch:\", X_A_batch)\n","  print(\"X_B_batch:\", X_B_batch)\n","\n","\n","  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n","    # Generator forward pass\n","    X_BA = Gen_BA(X_B_batch)\n","    X_AB = Gen_AB(X_A_batch)\n","\n","    # Discriminator forward pass\n","    Disc_A_real = Disc_A(X_A_batch)\n","    Disc_A_fake = Disc_A(X_BA)\n","    Disc_B_real = Disc_B(X_B_batch)\n","    Disc_B_fake = Disc_B(X_AB)\n","\n","    # Losses\n","    Loss_Disc_A = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_A_real, labels=tf.ones_like(Disc_A_real))) + \\\n","          tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_A_fake, labels=tf.zeros_like(Disc_A_fake)))\n","    Loss_Disc_B = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_B_real, labels=tf.ones_like(Disc_B_real))) + \\\n","          tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_B_fake, labels=tf.zeros_like(Disc_B_fake)))\n","\n","    # Apply gradient penalty\n","    lambda_gp = 1\n","    gp_A = gradient_penalty(Disc_A, X_A_batch, X_BA)\n","    gp_B = gradient_penalty(Disc_B, X_B_batch, X_AB)\n","\n","    Loss_Disc_A += lambda_gp * gp_A\n","    Loss_Disc_B += lambda_gp * gp_B\n","\n","    Disc_Loss = Loss_Disc_A + Loss_Disc_B\n","\n","    # Reconstruction and generator losses\n","    X_BAB = Gen_AB(X_BA)\n","    X_ABA = Gen_BA(X_AB)\n","\n","    Loss_Gen_A = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_B_fake, labels=tf.ones_like(Disc_B_fake)))\n","    Loss_Gen_B = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Disc_A_fake, labels=tf.ones_like(Disc_A_fake)))\n","    Loss_total = Loss_Gen_A + Loss_Gen_B\n","\n","    Loss_recon_A = tf.reduce_mean(10 * tf.abs(X_A_batch - X_ABA))\n","    Loss_recon_B = tf.reduce_mean(10 * tf.abs(X_B_batch - X_BAB))\n","    Loss_recon_total = Loss_recon_A + Loss_recon_B\n","\n","    Gen_Loss = Loss_total + Loss_recon_total\n","\n","  # Compute gradients\n","  Gen_gradients = gen_tape.gradient(Gen_Loss, Gen_param)\n","  Disc_gradients = disc_tape.gradient(Disc_Loss, Disc_param)\n","\n","  # Clip gradients to avoid them growing too large\n","  Gen_gradients, _ = tf.clip_by_global_norm(Gen_gradients, 5.0)\n","  Disc_gradients, _ = tf.clip_by_global_norm(Disc_gradients, 5.0)\n","\n","  print(\"Generator gradients:\", Gen_gradients)\n","  print(\"Discriminator gradients:\", Disc_gradients)\n","\n","  # Apply gradients\n","  Gen_optimizer.apply_gradients(zip(Gen_gradients, Gen_param))\n","  Disc_optimizer.apply_gradients(zip(Disc_gradients, Disc_param))\n","\n","  # Return the loss values (symbolic tensors)\n","  return Disc_Loss, Gen_Loss"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Download the Fashion MNIST dataset\n","!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","!wget http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","\n","# Check if the directory exists, create it if it doesn't\n","!if [ ! -d \"MNIST_Fashion\" ]; then mkdir MNIST_Fashion; fi\n","\n","# Move the downloaded files to the directory\n","!cp *.gz MNIST_Fashion/\n","\n","# Clearing the output\n","print(\"Dataset Download Complete! Please wait, the CycleGAN will begin training momentarily...\")\n","clear_output(wait=True)\n","\n","# Load Fashion MNIST data using TensorFlow 2.x API\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n","\n","# Assembling training data from two domains\n","mid = int(X_train.shape[0] / 2)\n","\n","# Real images Dataset 1\n","X_train_real = X_train[:mid]\n","\n","# Rotated images Dataset 2\n","X_train_rot = X_train[mid:].reshape(-1, 28, 28)\n","X_train_rot = scipy.ndimage.rotate(X_train_rot, 90, axes=(1, 2))\n","\n","# Random shuffling of data\n","def shuffle_data(x, size):\n","    start_index = np.random.randint(0, x.shape[0] - size)\n","    return x[start_index:start_index + size]\n","\n","def gradient_penalty(discriminator, real_data, fake_data):\n","    alpha = tf.random.uniform(shape=[batch_size, 1], minval=0., maxval=1.)\n","    interpolated = alpha * real_data + (1 - alpha) * fake_data\n","    with tf.GradientTape() as tape:\n","        tape.watch(interpolated)\n","        pred = discriminator(interpolated)\n","    grads = tape.gradient(pred, interpolated)\n","    slopes = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=1))\n","    gradient_penalty = tf.reduce_mean((slopes - 1.)**2)\n","    return gradient_penalty\n","\n","print(\"CycleGAN Training:\")\n","# Training loop, using the train_step function\n","for epoch in range(epochs):\n","    # Shuffle the training data for both domains\n","    X_A_batch = shuffle_data(X_train_real, batch_size)\n","    X_B_batch = shuffle_data(X_train_rot, batch_size)\n","\n","    # Perform a single training step\n","    train_step(X_A_batch, X_B_batch)\n","\n","    # Print the losses every 500 steps\n","    if epoch % 500 == 0:\n","      tf.print(f\"Epoch: {epoch}, Discriminator Loss: \", Disc_Loss, \", Generator Loss: \", Gen_Loss)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Testing\n","\n","n = 6\n","canvas1 = np.empty((28 * n, 28 * n))\n","canvas2 = np.empty((28 * n, 28 * n))\n","\n","for i in range(n):\n","\n","  test_A = shuffle_data{X_train_real, batch_size}\n","  text_B = shuffle_data(X_train_rot, batch_size)\n","\n","  # Generate A images from B\n","  out_A = sess.run(X_BA, feed_dict = {X_B:test_B})\n","  # Generate B images from A\n","  out_B = sess.run(X_AB, feed_dict={X_A:test_A})\n","\n","  for j in range(n):\n","    # Draw the generated digits\n","    canvas1[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = out_A[j].reshape([28, 28])\n","  for j in range(n):\n","    # Draw the generated digits\n","    canvas2[i * 28:(i + 1) * 28, j * 28:(j + 1) * 28] = out_B[j].reshape([28, 28])\n","\n","# One way of displaying\n","plt.figure(figsize=(n, n))\n","plt.imshow(canvas1, origin = \"upper\", cmap = \"gray\")\n","plt.show()\n","\n","plt.figure(figsize=(n, n))\n","plt.imshow(canvas2, origin = \"upper\", cmap = \"gray\")\n","plt.show()\n","\n","# Second way of displaying\n","f,ax = plt.subplots(1,2)\n","ax[0].imshow(canvas1, origin = \"upper\", cmap = \"gray\")\n","ax[1].imshow(canvas2, origin = \"upper\", cmap = \"gray\")\n","plt.show()"]}],"metadata":{"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":2}
