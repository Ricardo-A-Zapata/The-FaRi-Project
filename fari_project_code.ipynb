{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The FaRi Project - Project Code by Faith Villarreal and Ricky Zapata\n",
    "## Voice-To-Voice Voice Replication using a CycleGAN (Cycle Generative Adversarial Network)\n",
    "\n",
    "A Cycle Generative Adversarial Network (CycleGAN) is a deep learning model that extends the Generative Adversarial Network (GAN) framework by using two GANs. A GAN is made up of a Generator and a Discriminator: the Generator creates data, and the Discriminator attempts to differentiate between real and generated data. The two models improve through their adversarial interactionâ€”each one trying to \"outsmart\" the other.\n",
    "\n",
    "\n",
    "In a CycleGAN, the goal is to transform data from one domain to another (e.g., images of cats to dogs) without needing paired examples. The model consists of two GANs: one that maps from domain X to Y (cats to dogs) and one that maps back from Y to X (dogs to cats). A cycle consistency loss ensures that after converting an image from X to Y and then back to X, the final result resembles the original image, which helps prevent the Generator from producing arbitrary results. This process allows for the generation of more realistic and diverse outputs.\n",
    "\n",
    "\n",
    "In this project, we aim to utilize a Generative Adversarial network to allow train models on data of voices of prominent figures in culture and society in order to allow the user to input a recording of their voice and replicate a prominent figure of their choosing's voice to mimic them. Our project code is below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-Processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file_path, sr=22050):\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    return audio\n",
    "\n",
    "def extract_mel_spectrogram(audio, sr=22050, n_mels=128, hop_length=512, win_length=1024):\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length, win_length=win_length)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "def inverse_mel_spectrogram(mel_spec, sr=22050, n_fft=1024, hop_length=512, win_length=1024):\n",
    "    mel_spec = librosa.db_to_power(mel_spec)\n",
    "    return librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, hop_length=hop_length, win_length=win_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CycleGAN (Cycle Generative Adversarial Network) Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm1d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm1d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=128, num_residual_blocks=6):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        model = [\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm1d(64),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        \n",
    "        in_channels = 64\n",
    "        out_channels = 128\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.InstanceNorm1d(out_channels),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        \n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResBlock(in_channels)]\n",
    "        \n",
    "        out_channels = in_channels // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose1d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.InstanceNorm1d(out_channels),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "            out_channels //= 2\n",
    "        \n",
    "        model += [nn.Conv1d(in_channels, input_dim, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [nn.Conv1d(512, 1, kernel_size=4, stride=1, padding=1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training our CycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cycleGAN(data_loader, num_epochs=100):\n",
    "    # Initialize models\n",
    "    generator_A2B = Generator()\n",
    "    generator_B2A = Generator()\n",
    "    discriminator_A = Discriminator()\n",
    "    discriminator_B = Discriminator()\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.MSELoss()\n",
    "    cycle_loss = nn.L1Loss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(list(generator_A2B.parameters()) + list(generator_B2A.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D_A = optim.Adam(discriminator_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D_B = optim.Adam(discriminator_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_A, real_B) in enumerate(data_loader):\n",
    "            \n",
    "            # Train Generators A2B and B2A\n",
    "            optimizer_G.zero_grad()\n",
    "            identity_A = generator_B2A(real_A)\n",
    "            identity_B = generator_A2B(real_B)\n",
    "            loss_identity_A = cycle_loss(identity_A, real_A)\n",
    "            loss_identity_B = cycle_loss(identity_B, real_B)\n",
    "            \n",
    "            fake_B = generator_A2B(real_A)\n",
    "            fake_A = generator_B2A(real_B)\n",
    "            pred_fake_B = discriminator_B(fake_B)\n",
    "            pred_fake_A = discriminator_A(fake_A)\n",
    "            loss_GAN_A2B = adversarial_loss(pred_fake_B, torch.ones_like(pred_fake_B))\n",
    "            loss_GAN_B2A = adversarial_loss(pred_fake_A, torch.ones_like(pred_fake_A))\n",
    "            \n",
    "            recovered_A = generator_B2A(fake_B)\n",
    "            recovered_B = generator_A2B(fake_A)\n",
    "            loss_cycle_A = cycle_loss(recovered_A, real_A)\n",
    "            loss_cycle_B = cycle_loss(recovered_B, real_B)\n",
    "            \n",
    "            loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_A + loss_cycle_B\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Train Discriminator A\n",
    "            optimizer_D_A.zero_grad()\n",
    "            pred_real_A = discriminator_A(real_A)\n",
    "            pred_fake_A = discriminator_A(fake_A.detach())\n",
    "            loss_D_A_real = adversarial_loss(pred_real_A, torch.ones_like(pred_real_A))\n",
    "            loss_D_A_fake = adversarial_loss(pred_fake_A, torch.zeros_like(pred_fake_A))\n",
    "            loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "            \n",
    "            # Train Discriminator B\n",
    "            optimizer_D_B.zero_grad()\n",
    "            pred_real_B = discriminator_B(real_B)\n",
    "            pred_fake_B = discriminator_B(fake_B.detach())\n",
    "            loss_D_B_real = adversarial_loss(pred_real_B, torch.ones_like(pred_real_B))\n",
    "            loss_D_B_fake = adversarial_loss(pred_fake_B, torch.zeros_like(pred_fake_B))\n",
    "            loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "            \n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss G: {loss_G.item()}, Loss D_A: {loss_D_A.item()}, Loss D_B: {loss_D_B.item()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Griffin-Lim Vocoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def griffin_lim(mel_spec, n_fft=1024, hop_length=512, win_length=1024, iterations=60):\n",
    "    mel_spec = librosa.db_to_power(mel_spec)\n",
    "    return librosa.griffinlim(mel_spec, n_iter=iterations, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "# Example of converting generated Mel-spectrogram to waveform\n",
    "def convert_to_audio(fake_mel_spec):\n",
    "    output_audio = griffin_lim(fake_mel_spec.detach().cpu().numpy())\n",
    "    librosa.output.write_wav('converted_voice.wav', output_audio, sr=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, audio_paths_A, audio_paths_B, sr=22050):\n",
    "        self.audio_paths_A = audio_paths_A  # List of file paths for user's voice\n",
    "        self.audio_paths_B = audio_paths_B  # List of file paths for celebrity's voice\n",
    "        self.sr = sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.audio_paths_A), len(self.audio_paths_B))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_A = load_audio(self.audio_paths_A[idx], sr=self.sr)\n",
    "        audio_B = load_audio(self.audio_paths_B[idx], sr=self.sr)\n",
    "\n",
    "        mel_A = extract_mel_spectrogram(audio_A)\n",
    "        mel_B = extract_mel_spectrogram(audio_B)\n",
    "\n",
    "        mel_A = torch.tensor(mel_A, dtype=torch.float32).unsqueeze(0)  # Add channel dimension for 1D convolution\n",
    "        mel_B = torch.tensor(mel_B, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return mel_A, mel_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Paths to datasets\n",
    "    user_voice_dir = 'data/user_voice/'\n",
    "    celebrity_voice_dir = 'data/celebrity_voice/'\n",
    "    \n",
    "    audio_paths_A = [os.path.join(user_voice_dir, f) for f in os.listdir(user_voice_dir)]\n",
    "    audio_paths_B = [os.path.join(celebrity_voice_dir, f) for f in os.listdir(celebrity_voice_dir)]\n",
    "    \n",
    "    # Create dataset and data loader\n",
    "    dataset = VoiceDataset(audio_paths_A, audio_paths_B)\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Train the CycleGAN model\n",
    "    print(\"Training CycleGAN model...\")\n",
    "    train_cycleGAN(data_loader, num_epochs=100)\n",
    "\n",
    "    # Example: converting a single sample after training\n",
    "    print(\"Converting example voice...\")\n",
    "    audio = load_audio('data/user_voice/example.wav')\n",
    "    mel_spectrogram = extract_mel_spectrogram(audio)\n",
    "    mel_spectrogram_tensor = torch.tensor(mel_spectrogram, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Load trained model and perform conversion (assuming the model is trained)\n",
    "    generator_A2B = Generator()\n",
    "    generator_A2B.eval()  # Set the generator to evaluation mode\n",
    "    fake_mel_spectrogram = generator_A2B(mel_spectrogram_tensor)\n",
    "\n",
    "    # Convert the Mel-spectrogram back to audio\n",
    "    convert_to_audio(fake_mel_spectrogram)\n",
    "\n",
    "    print(\"Converted voice saved to 'converted_voice.wav'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credits and Thanks\n",
    "\n",
    "## In search of resources to learn more about deep learning, adversarial networks, and generative adversarial networks for this project, the following videos/resources were very helpful so I want to show appreciation and catalog them below:\n",
    "\n",
    "[Deep Learning Crash Course for Beginners]( https://www.youtube.com/watch?v=VyWAvY2CF9c&t=3387s&ab_channel=freeCodeCamp.org) by freeCodeCamp.org\n",
    "\n",
    "[Generative Adversarial Networks (GANs) - Computerphile](https://www.youtube.com/watch?v=Sw9r8CL98N0&ab_channel=Computerphile) by Computerphile\n",
    "\n",
    "[Zebras, Horses & CycleGAN - Computerphile](https://www.youtube.com/watch?v=T-lBMrjZ3_0&ab_channel=Computerphile) by Computerphile"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
