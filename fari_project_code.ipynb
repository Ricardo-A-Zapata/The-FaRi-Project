{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voice-To-Voice Machine Learning model by Faith Villarreal and Ricky Zapata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Here is some starter code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb Cell 3\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \u001b[39m# Load the dataset (Replace with the actual path to your dataset)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     dataset_path \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mpath_to_dataset\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     X, y \u001b[39m=\u001b[39m load_dataset(dataset_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39m# Split the dataset into training and testing sets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     X_train, X_test, y_train, y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "\u001b[1;32m/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m y \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Assuming dataset folder structure as 'dataset_path/class_name/audio_files'\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mfor\u001b[39;00m class_name \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(dataset_path):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     class_folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(dataset_path, class_name)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ricardozapata/FaRi_Project/fari_project_code.ipynb#W2sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mfor\u001b[39;00m audio_file \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39mlistdir(class_folder):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_dataset'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import librosa\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# ================== DATA PREPROCESSING ==================\n",
    "\n",
    "def load_audio(file_path, sr=22050):\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    return audio\n",
    "\n",
    "def extract_mel_spectrogram(audio, sr=22050, n_mels=128, hop_length=512, win_length=1024):\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio, sr=sr, n_mels=n_mels, hop_length=hop_length, win_length=win_length)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "    return mel_spec_db\n",
    "\n",
    "def inverse_mel_spectrogram(mel_spec, sr=22050, n_fft=1024, hop_length=512, win_length=1024):\n",
    "    mel_spec = librosa.db_to_power(mel_spec)\n",
    "    return librosa.feature.inverse.mel_to_audio(mel_spec, sr=sr, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "# ================== CYCLEGAN ARCHITECTURE ==================\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm1d(channels),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv1d(channels, channels, kernel_size=3, padding=1),\n",
    "            nn.InstanceNorm1d(channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.conv(x)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim=128, num_residual_blocks=6):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        model = [\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm1d(64),\n",
    "            nn.ReLU(True)\n",
    "        ]\n",
    "        \n",
    "        in_channels = 64\n",
    "        out_channels = 128\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.InstanceNorm1d(out_channels),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "            out_channels *= 2\n",
    "        \n",
    "        for _ in range(num_residual_blocks):\n",
    "            model += [ResBlock(in_channels)]\n",
    "        \n",
    "        out_channels = in_channels // 2\n",
    "        for _ in range(2):\n",
    "            model += [\n",
    "                nn.ConvTranspose1d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "                nn.InstanceNorm1d(out_channels),\n",
    "                nn.ReLU(True)\n",
    "            ]\n",
    "            in_channels = out_channels\n",
    "            out_channels //= 2\n",
    "        \n",
    "        model += [nn.Conv1d(in_channels, input_dim, kernel_size=7, padding=3), nn.Tanh()]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim=128):\n",
    "        super(Discriminator, self).__init__()\n",
    "        model = [\n",
    "            nn.Conv1d(input_dim, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv1d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv1d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.InstanceNorm1d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [\n",
    "            nn.Conv1d(256, 512, kernel_size=4, stride=1, padding=1),\n",
    "            nn.InstanceNorm1d(512),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        ]\n",
    "        \n",
    "        model += [nn.Conv1d(512, 1, kernel_size=4, stride=1, padding=1)]\n",
    "        \n",
    "        self.model = nn.Sequential(*model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# ================== TRAINING LOOP ==================\n",
    "\n",
    "def train_cycleGAN(data_loader, num_epochs=100):\n",
    "    # Initialize models\n",
    "    generator_A2B = Generator()\n",
    "    generator_B2A = Generator()\n",
    "    discriminator_A = Discriminator()\n",
    "    discriminator_B = Discriminator()\n",
    "\n",
    "    # Loss functions\n",
    "    adversarial_loss = nn.MSELoss()\n",
    "    cycle_loss = nn.L1Loss()\n",
    "\n",
    "    # Optimizers\n",
    "    optimizer_G = optim.Adam(list(generator_A2B.parameters()) + list(generator_B2A.parameters()), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D_A = optim.Adam(discriminator_A.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "    optimizer_D_B = optim.Adam(discriminator_B.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (real_A, real_B) in enumerate(data_loader):\n",
    "            \n",
    "            # Train Generators A2B and B2A\n",
    "            optimizer_G.zero_grad()\n",
    "            identity_A = generator_B2A(real_A)\n",
    "            identity_B = generator_A2B(real_B)\n",
    "            loss_identity_A = cycle_loss(identity_A, real_A)\n",
    "            loss_identity_B = cycle_loss(identity_B, real_B)\n",
    "            \n",
    "            fake_B = generator_A2B(real_A)\n",
    "            fake_A = generator_B2A(real_B)\n",
    "            pred_fake_B = discriminator_B(fake_B)\n",
    "            pred_fake_A = discriminator_A(fake_A)\n",
    "            loss_GAN_A2B = adversarial_loss(pred_fake_B, torch.ones_like(pred_fake_B))\n",
    "            loss_GAN_B2A = adversarial_loss(pred_fake_A, torch.ones_like(pred_fake_A))\n",
    "            \n",
    "            recovered_A = generator_B2A(fake_B)\n",
    "            recovered_B = generator_A2B(fake_A)\n",
    "            loss_cycle_A = cycle_loss(recovered_A, real_A)\n",
    "            loss_cycle_B = cycle_loss(recovered_B, real_B)\n",
    "            \n",
    "            loss_G = loss_identity_A + loss_identity_B + loss_GAN_A2B + loss_GAN_B2A + loss_cycle_A + loss_cycle_B\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "            \n",
    "            # Train Discriminator A\n",
    "            optimizer_D_A.zero_grad()\n",
    "            pred_real_A = discriminator_A(real_A)\n",
    "            pred_fake_A = discriminator_A(fake_A.detach())\n",
    "            loss_D_A_real = adversarial_loss(pred_real_A, torch.ones_like(pred_real_A))\n",
    "            loss_D_A_fake = adversarial_loss(pred_fake_A, torch.zeros_like(pred_fake_A))\n",
    "            loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
    "            loss_D_A.backward()\n",
    "            optimizer_D_A.step()\n",
    "            \n",
    "            # Train Discriminator B\n",
    "            optimizer_D_B.zero_grad()\n",
    "            pred_real_B = discriminator_B(real_B)\n",
    "            pred_fake_B = discriminator_B(fake_B.detach())\n",
    "            loss_D_B_real = adversarial_loss(pred_real_B, torch.ones_like(pred_real_B))\n",
    "            loss_D_B_fake = adversarial_loss(pred_fake_B, torch.zeros_like(pred_fake_B))\n",
    "            loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
    "            loss_D_B.backward()\n",
    "            optimizer_D_B.step()\n",
    "            \n",
    "        print(f'Epoch [{epoch}/{num_epochs}], Loss G: {loss_G.item()}, Loss D_A: {loss_D_A.item()}, Loss D_B: {loss_D_B.item()}')\n",
    "\n",
    "# ================== GRIFFIN-LIM VOCODER ==================\n",
    "\n",
    "def griffin_lim(mel_spec, n_fft=1024, hop_length=512, win_length=1024, iterations=60):\n",
    "    mel_spec = librosa.db_to_power(mel_spec)\n",
    "    return librosa.griffinlim(mel_spec, n_iter=iterations, hop_length=hop_length, win_length=win_length)\n",
    "\n",
    "# Example of converting generated Mel-spectrogram to waveform\n",
    "def convert_to_audio(fake_mel_spec):\n",
    "    output_audio = griffin_lim(fake_mel_spec.detach().cpu().numpy())\n",
    "    librosa.output.write_wav('converted_voice.wav', output_audio, sr=22050)\n",
    "\n",
    "# ================== DATA LOADER AND MAIN ==================\n",
    "\n",
    "class VoiceDataset(Dataset):\n",
    "    def __init__(self, audio_paths_A, audio_paths_B, sr=22050):\n",
    "        self.audio_paths_A = audio_paths_A  # List of file paths for user's voice\n",
    "        self.audio_paths_B = audio_paths_B  # List of file paths for celebrity's voice\n",
    "        self.sr = sr\n",
    "\n",
    "    def __len__(self):\n",
    "        return min(len(self.audio_paths_A), len(self.audio_paths_B))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio_A = load_audio(self.audio_paths_A[idx], sr=self.sr)\n",
    "        audio_B = load_audio(self.audio_paths_B[idx], sr=self.sr)\n",
    "\n",
    "        mel_A = extract_mel_spectrogram(audio_A)\n",
    "        mel_B = extract_mel_spectrogram(audio_B)\n",
    "\n",
    "        mel_A = torch.tensor(mel_A, dtype=torch.float32).unsqueeze(0)  # Add channel dimension for 1D convolution\n",
    "        mel_B = torch.tensor(mel_B, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "        return mel_A, mel_B\n",
    "\n",
    "def main():\n",
    "    # Paths to datasets\n",
    "    user_voice_dir = 'data/user_voice/'\n",
    "    celebrity_voice_dir = 'data/celebrity_voice/'\n",
    "    \n",
    "    audio_paths_A = [os.path.join(user_voice_dir, f) for f in os.listdir(user_voice_dir)]\n",
    "    audio_paths_B = [os.path.join(celebrity_voice_dir, f) for f in os.listdir(celebrity_voice_dir)]\n",
    "    \n",
    "    # Create dataset and data loader\n",
    "    dataset = VoiceDataset(audio_paths_A, audio_paths_B)\n",
    "    data_loader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "\n",
    "    # Train the CycleGAN model\n",
    "    print(\"Training CycleGAN model...\")\n",
    "    train_cycleGAN(data_loader, num_epochs=100)\n",
    "\n",
    "    # Example: converting a single sample after training\n",
    "    print(\"Converting example voice...\")\n",
    "    audio = load_audio('data/user_voice/example.wav')\n",
    "    mel_spectrogram = extract_mel_spectrogram(audio)\n",
    "    mel_spectrogram_tensor = torch.tensor(mel_spectrogram, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Load trained model and perform conversion (assuming the model is trained)\n",
    "    generator_A2B = Generator()\n",
    "    generator_A2B.eval()  # Set the generator to evaluation mode\n",
    "    fake_mel_spectrogram = generator_A2B(mel_spectrogram_tensor)\n",
    "\n",
    "    # Convert the Mel-spectrogram back to audio\n",
    "    convert_to_audio(fake_mel_spectrogram)\n",
    "\n",
    "    print(\"Converted voice saved to 'converted_voice.wav'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
